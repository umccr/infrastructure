[aws]
aws_region_name = ap-southeast-2

[global]
cluster_template = tothill 
update_check     = true
sanity_check     = true

## Clusters
[cluster tothill]
base_os               = alinux2
key_name              = romanvg 
vpc_settings          = tothill_network
efs_settings          = awselasticfs
s3_read_resource      = *
master_instance_type  = t2.medium
# Until the SLURM_COMPUTE_NODE_REAL_MEM is part of a ssm-parameter this will
# compute_instance_type will need to be uniform across each cluster template
additional_iam_policies = arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore
scheduler             = slurm
pre_install           = s3://tothill-parallel-cluster-dev/bootstrap/pre_install.sh
post_install          = s3://tothill-parallel-cluster-dev/bootstrap/post_install.sh
custom_ami            = ami-074416eece29e32ec
master_root_volume_size  = 45  # Gb - for multiple conda installations
compute_root_volume_size = 60  # Gb - for multiple docker containers to be pulled down to run commands
# Set the queue
queue_settings        = compute


[cluster umccr_dev]
base_os               = alinux2
vpc_settings          = umccr_dev_network
efs_settings          = awselasticfs
s3_read_resource      = *
key_name              = alexis-wfh-dev
# Need something substantial to hold the slurm database
master_instance_type  = t2.medium
# Using additional_iam_policies over ec2_iam_role
# Add SSM policy, so we can keep port 22 closed
additional_iam_policies = arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore
scheduler             = slurm
# Basic ami with installations of
# R, python3.8, conda, pip, ruby, golang, rust
# conda has been initialised to the ec2-user
# See the ami readme for recreation of the custom_ami
custom_ami            = ami-0cf68ccca246d4f2f
pre_install           = s3://umccr-research-dev/parallel-cluster/bootstrap/pre_install.sh
post_install          = s3://umccr-research-dev/parallel-cluster/bootstrap/post_install.sh
master_root_volume_size  = 45  # Gb - for multiple conda installations
compute_root_volume_size = 60  # Gb - for multiple docker containers to be pulled down to run commands
# Set the queue
queue_settings        = compute

[cluster umccr_dev_fsx]
base_os               = alinux2
vpc_settings          = umccr_dev_network
fsx_settings          = lustrefs
s3_read_resource      = *
key_name              = alexis-wfh-dev
# Need something substantial to hold the slurm database
master_instance_type  = t2.medium
# Using additional_iam_policies over ec2_iam_role
# Add SSM policy, so we can keep port 22 closed
additional_iam_policies = arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore
scheduler             = slurm
# Basic ami with installations of
# R, python3.8, conda, pip, ruby, golang, rust
# conda has been initialised to the ec2-user
# See the ami readme for recreation of the custom_ami
custom_ami            = ami-0cf68ccca246d4f2f
pre_install           = s3://umccr-research-dev/parallel-cluster/bootstrap/pre_install.sh
post_install          = s3://umccr-research-dev/parallel-cluster/bootstrap/post_install.sh
master_root_volume_size = 45   # Gb - for multiple conda installations
compute_root_volume_size = 60  # Gb - for multiple docker containers to be pulled down to run commands
# Set the queue
queue_settings        = compute

## Networks
[vpc tothill_network]
vpc_id = vpc-06daaa8ca8e5c853e
master_subnet_id = 	subnet-061d824f3056967b5
use_public_ips = true
additional_sg  = sg-0b4296d2fae709454

[vpc umccr_dev_network]
# Default vpc for dev account
vpc_id = vpc-00eafc63c0dfca266
# Compute subnet ids take the same subnet as the master
# Our default public subnet
master_subnet_id = subnet-0fab038b0341872f1
# Elastic IP address is associated to the master instance.
use_public_ips = true
# sg-0ca5bdaab39885649: Security group where the slurm accounting database sits
additional_sg         = sg-0ca5bdaab39885649

[aliases]
ssh = ssh {CFN_USER}@{MASTER_IP} {ARGS}

## Filesystems
[efs awselasticfs]
shared_dir       = efs
encrypted        = true
performance_mode = generalPurpose

[fsx lustrefs]
# The mountpoint of the filesystem
shared_dir                = /fsx
# Storage available in GB
storage_capacity          = 1200
# Default system
deployment_type           = SCRATCH_2

## Resource partitions / queues
[queue compute]
compute_resource_settings = himem,hicpu
compute_type = spot

## Compute resources
[compute_resource himem]
instance_type = m5.4xlarge  # 16 Cpus and 64 Gbs of RAM

[compute_resource hicpu]
instance_type = c5.4xlarge  # 16 Cpus and 32 Gbs of RAM