# This is an example of how you can use Singularity containers with Cromwell
# *This is not a complete configuration file!* The
# content here should be copy pasted into the backend -> providers section
# of the cromwell.examples.conf in the root of the repository. You should
# uncomment lines that you want to define, and read carefully to customize
# the file. If you have any questions, please open an issue at
# https://www.github.com/broadinstitute/cromwell/issues

# Documentation
# https://cromwell.readthedocs.io/en/develop/tutorials/Containers/#job-schedulers

include required(classpath("application"))

akka {
  http.server.request-timeout = 120s
}

system {
  io {
    number-of-requests = 100000
    per = 100 seconds
    number-of-attempts = 50
  }

  max-concurrent-workflows = 10
  new-workflow-poll-rate = 1
  max-workflow-launch-count = 10
  abort-jobs-on-terminate = true

}

workflow-options {
  workflow-log-temporary = false
  workflow-log-dir = "workflow_logs"
}

docker {

  hash-lookup {

    // /!\ Attention /!\

    // If you disable this call caching will be disabled for jobs with floating docker tags !
    enabled = false

    // Set this to match your available quota against the Google Container Engine API
    // This setting is deprecated, prefer using the gcr throttle configuration as shown below
    gcr-api-queries-per-100-seconds = 1000

    // Time in minutes before an entry expires from the docker hashes cache and needs to be fetched again
    cache-entry-ttl = "20 minutes"

    // Maximum number of elements to be kept in the cache. If the limit is reached, old elements will be removed from the cache
    cache-size = 200

    // How should docker hashes be looked up. Possible values are "local" and "remote"
    // "local": Lookup hashes on the local docker daemon using the cli
    // "remote": Lookup hashes on docker hub and gcr
    method = "remote"

    // For docker repositories that support the version 2 of the manifest schema, Cromwell can retrieve the compressed size
    // of the images. This compressed size is smaller than the actual size needed on disk when the docker image is pulled.
    // This factor is used to multiply the compressed size and get an approximation of the size needed on disk so that the
    // disk can be allocated appropriately.
    // See https://github.com/docker/hub-feedback/issues/331
    size-compression-factor = 6.0

    // Retry configuration for http requests - across all registries
    max-time-between-retries = 1 minute
    max-retries = 3

    // Supported registries (dockerhub, gcr, quay) can have additional configuration set separately
    gcr {
      // Example of how to configure throttling, available for all supported registries
      // throttle {
      //   number-of-requests = 1000
      //   per = 100 seconds
      // }
      // How many threads to allocate for gcr related work
      num-threads = 10
    }
    dockerhub.num-threads = 10
    quay.num-threads = 10
  }
}

backend {
  default = slurm

  providers {
    slurm {
      actor-factory = "cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"
      # File system configuration.
      filesystems {
        # For SFS backends, the "local" configuration specifies how files are handled.
        local {
          # Try to hard link (ln), then soft-link (ln -s), and if both fail, then copy the files.
          localization: [
            "hard-link", "cached-copy"
          ]
          # Call caching strategies
          caching {
            # When copying a cached result, what type of file duplication should occur. Attempted in the order listed below:
            duplication-strategy: [
              "hard-link", "cached-copy"
            ]
            # Possible values: file, path, path+modtime
            # "file" will compute an md5 hash of the file content.
            # "path" will compute an md5 hash of the file path. This strategy will only be effective if the duplication-strategy (above) is set to "soft-link",
            # in order to allow for the original file path to be hashed.
            # "path+modtime" will compute an md5 hash of the file path and the last modified time. The same conditions as for "path" apply here.
            # Default: file
            hashing-strategy: "path+modtime"
            # When true, will check if a sibling file with the same name and the .md5 extension exists, and if it does, use the content of this file as a hash.
            # If false or the md5 does not exist, will proceed with the above-defined hashing strategy.
            check-sibling-md5: false
          }
        }
      }
      config {

        concurrent-job-limit = 2000

        runtime-attributes = """
          String run_time = '1-0'
          Int cpu = 1
          Int memory_mb = 8000
          String? docker
        """

        submit-docker = """

          # Submit the script to sbatch
          # Env vars go under --export
          # No other env vars are inherited by the script
          sbatch \
            --job-name=${job_name} \
            --chdir=${cwd} \
            --output=${cwd}/execution/stdout.batch \
            --error=${cwd}/execution/stderr.batch \
            --time=${run_time} \
            --cpus-per-task=${cpu} \
            --mem-per-cpu=$((${memory_mb}/${cpu}))M \
            --partition=compute \
            --export=IMAGE_NAME=${docker_cid},IMAGE_PATH=${docker},CWD=${cwd},DOCKER_CWD=${docker_cwd},JOB_SHELL=${job_shell},SCRIPT_PATH=${script},DOCKER_SCRIPT_PATH=${docker_script} \
            /opt/cromwell/scripts/submit_to_sbatch.sh
        """

        kill = "scancel ${job_id}"
        check-alive = "squeue -j ${job_id}"
        job-id-regex = "(\\d+).*"
        root = "executions"
        dockerRoot = "/cromwell/executions"
      }
    }
  }
}